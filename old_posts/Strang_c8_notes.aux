\relax 
\citation{strang_computational_2012}
\citation{strang_computational_2012}
\citation{lay_linear_2012}
\@writefile{toc}{\contentsline {section}{\numberline {1}Least Squares}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Refresher on Orthogonality}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Orthogonal Complements in $\mathds  {R}^n$}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Fundamental subspaces of $A$. From Lay.}}{3}}
\newlabel{fig:layfundamentalsubspaces}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Primal and Dual Problems}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Ordinary least squares. From Strang.}}{4}}
\newlabel{fig:strangprojectionlsq}{{2}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}"Saddle Point" Equations}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}"Primal-Dual" Equations}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}An Example}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Weighted Least Squares}{6}}
\newlabel{Weighted Least Squares}{{2}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Weighted least squares.}}{7}}
\newlabel{fig:strangprojectionwlsq}{{3}{7}}
\newlabel{wlsq1}{{53}{8}}
\newlabel{wlsq2}{{54}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.0.1}Duality and Weak Duality}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Geometric interpretation of weak duality. From Strang.}}{9}}
\newlabel{fig:strangwsqweakduality}{{4}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Minimizing with Constraints using Lagrange's Method}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces System with two linear springs and one mass.}}{10}}
\newlabel{fig:strangconstrainedsprings}{{5}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Geometry of the method}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Geometric depiction of $E(w_1, w_2)$ and constraint $w_1 - w_2 = f$.}}{11}}
\newlabel{fig:strangconstrainedgeo}{{6}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The Fundamental Problem}{13}}
\newlabel{fundamental_pLpw}{{79}{13}}
\newlabel{fundamental_pLpu}{{80}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Regularized Least Squares}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Example: Large Penalty Enforced $Bu=d$}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The Pseudoinverse}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}What is $u^{+}$?}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces $Au$ takes vectors $u = u_{\textrm  {row}} + u_{\textrm  {null}}$ to the column space of $A$. Since $u_{\textrm  {row}}$ and $u_{\textrm  {null}}$ are orthogonal, a non-zero $u_{\textrm  {null}}$ increases $\delimiter 69640972 u \delimiter 86418188 ^{2}$. Thus, $u^{+} = u_{\textrm  {row}}$}}{15}}
\newlabel{fig:strangpseudoinverse}{{7}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Singular Value Decomposition (SVD)}{15}}
\@writefile{toc}{\contentsline {paragraph}{Diagonalization of Symmetric Matrices \\}{16}}
\@writefile{toc}{\contentsline {paragraph}{Spectral decomposition of a Symmetric Matrix \\}{16}}
\@writefile{toc}{\contentsline {paragraph}{SVD of an $m \times n$ matrix $A$ \\}{17}}
\@writefile{toc}{\contentsline {paragraph}{SVD \\}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Computing $u^{+}$ with SVD}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Computing $u^{+}$ with Tychonov Regularization}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Tychonov Regularization}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Error bounds in $\mathaccentV {hat}05E{u}$}{20}}
\@writefile{toc}{\contentsline {paragraph}{Error bounds when $A$ is a scalar. \\ }{20}}
\newlabel{errorbounds}{{93}{20}}
\newlabel{errorbounds_scalar_nonoise}{{96}{20}}
\newlabel{errorbound_scalar_withnoise}{{100}{20}}
\@writefile{toc}{\contentsline {paragraph}{Error bounds when $A$ is a matrix. \\}{21}}
\newlabel{errorbound_matrix_nonoise}{{103}{21}}
\bibstyle{unsrt}
\bibdata{./bib/strang_and_lay_linalg}
\bibcite{lay_linear_2012}{1}
\newlabel{errorbound_matrix_noise_final}{{109}{22}}
\@writefile{toc}{\contentsline {paragraph}{What is a good $\alpha $? \\}{22}}
\bibcite{strang_computational_2012}{2}
