\documentclass[]{article}
\usepackage{amsmath, amssymb, graphicx, float, dsfont}

%opening
\title{Notes for "Statistics Informed Decisions using Data"}
\author{Delbert Yip}

\begin{document}

\maketitle

\begin{abstract}
	These notes covers several chapters from "Statistics Informed Decisions using Data," focusing on practical applications. 
\end{abstract}

\section{Probability Distributions}
\textbf{Poisson distribution}
A random variable $X$, the number of successes in a fixed interval, follows a Poisson process if:
\begin{itemize}
	\item The probability of 2 or more successes in a sufficiently small subinterval, is 0.
	\item The probability of success is the same for any two intervals of equal length. 
	\item The number of successes in any interval is independent of the number of successes in any other intervals, as long as said intervals do not overlap. 
\end{itemize}

A Poisson process is described by:
\begin{equation}
P(x) = \frac{(\lambda t)^x}{x!} \exp(-\lambda t), \quad x=0,1,...n
\end{equation}

Where $\lambda$ is the average number of occurrences of the event in an interval of length 1. 

\section{Inference}
\subsection{Hypothesis Tests}
We try to find support for the \textbf{alternative hypothesis}, $H_1$. The \textbf{null hypothesis}, $H_0$, is assumed true until refuted by evidence, and assumes no change/difference/etc. Three ways to set up null and alternative hypotheses:
\begin{enumerate}
	\item Two-tailed test: equal versus not equal
	\begin{align*}
		H_0 &= x \\
		H_1 &\neq x
	\end{align*}
	
	\item Left-tailed test: equal versus less than 
	\begin{align*}
	H_0 &= x \\
	H_1 &< x
	\end{align*}
	
	\item Right-tailed test: equal versus greater than 
	\begin{align*}
	H_0 &= x \\
	H_1 &> x
	\end{align*}
\end{enumerate}

$H_0$ is always a statement of equality. Left- and right-tailed tests are a.k.a one-tailed tests. There are two types of errors in hypothesis testing: Type I (false negative, $H_0$ is rejected when $H_0$ is actually true) and Type II (false positive, $H_0$ is not rejected while $H_1$ is true.) Importantly, the \textbf{level of significance, $\alpha$} is the \textit{probability of making a Type I error/false negative}.The  choice of $\alpha$ depends on the consequences of making a Type I error, or false negative. On the other hand, $\beta$ is the probability of making a Type II error/false positive. There is an inverse relation between $\alpha$ and $\beta$: when $\alpha$ is small, $\beta$ is large. 

\end{document}
